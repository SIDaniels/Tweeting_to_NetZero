{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /Users/sidan/opt/anaconda3/envs/classification/lib/python3.8/site-packages (4.10.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /Users/sidan/opt/anaconda3/envs/classification/lib/python3.8/site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /Users/sidan/opt/anaconda3/envs/classification/lib/python3.8/site-packages (from tweepy) (2.27.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /Users/sidan/opt/anaconda3/envs/classification/lib/python3.8/site-packages (from tweepy) (3.2.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sidan/opt/anaconda3/envs/classification/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sidan/opt/anaconda3/envs/classification/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/sidan/opt/anaconda3/envs/classification/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sidan/opt/anaconda3/envs/classification/lib/python3.8/site-packages (from requests<3,>=2.27.0->tweepy) (2021.10.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain Third Week of Data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client( bearer_token=#insert token,\n",
    "                          wait_on_rate_limit=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://twittercommunity.com/t/introducing-the-sort-order-parameter-for-search-endpoints-in-the-twitter-api-v2/166377\n",
    "#https://twittercommunity.com/t/user-fields-not-working/151981/2\n",
    "\n",
    "query = '(carbon credits) OR (carbon credit) OR (carbon market) OR (carbon markets) OR (carbon trading) OR (carbon trade) OR (carbon offset) OR (carbon offsets) OR #carboncredit OR #carboncredits OR #carbonmarket OR #carbonmarkets OR #carbontrade OR #carbontrading OR #carbonoffset OR #carbonoffsets -is:retweet lang=en'\n",
    "tweets=tweepy.Paginator(client.search_recent_tweets,query=query, tweet_fields=['context_annotations','created_at','public_metrics','author_id','geo'], \n",
    "                                                #expansions= ['author_id','geo.place_id'],\n",
    "                                                #place_fields=['id','country','country_code','name'],\n",
    "                                                #user_fields=['name','username','description','location','public_metrics','verified'],\n",
    "                                                #sort_order='relevancy',\n",
    "                                                max_results=100,limit=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paged_results_df_wk3 = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Iterate through each page of results\n",
    "for n,page in enumerate(tweets):  \n",
    "\n",
    "  # Print the current page number\n",
    "  print(\"PAGE NUMBER: \",n)\n",
    "  \n",
    "  # Iterate through each tweet in the page.data, convert to a dataframe, and \n",
    "  # then append to the paged_results_df above.\n",
    "  for tweet in page.data:\n",
    "    temp_df_wk3 = pd.json_normalize(tweet.data, sep=\"_\")\n",
    "    paged_results_df_wk3 = paged_results_df_wk3.append(temp_df_wk3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://twittercommunity.com/t/introducing-the-sort-order-parameter-for-search-endpoints-in-the-twitter-api-v2/166377\n",
    "#https://twittercommunity.com/t/user-fields-not-working/151981/2\n",
    "\n",
    "query = '(net zero) OR #netzero -is:retweet lang=en'\n",
    "tweets=tweepy.Paginator(client.search_recent_tweets,query=query, tweet_fields=['context_annotations','created_at','public_metrics','author_id','geo'], \n",
    "                                                expansions= ['author_id','geo.place_id'],\n",
    "                                                #place_fields=['id','country','country_code','name'],\n",
    "                                                user_fields=['name','username','description','location','public_metrics','verified'],\n",
    "                                                #sort_order='relevancy',\n",
    "                                                max_results=100,limit=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paged_results_df_test_wk3 = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Iterate through each page of results\n",
    "for n,page in enumerate(tweets):  \n",
    "\n",
    "  # Print the current page number\n",
    "  print(\"PAGE NUMBER: \",n)\n",
    "  \n",
    "  # Iterate through each tweet in the page.data, convert to a dataframe, and \n",
    "  # then append to the paged_results_df above.\n",
    "  for tweet in page.data:\n",
    "    temp_df_test_wk3 = pd.json_normalize(tweet.data, sep=\"_\")\n",
    "    paged_results_df_test_wk3 = paged_results_df_test_wk3.append(temp_df_test_wk3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '(carbon neutral) OR #carbonneutral -is:retweet lang=en'\n",
    "tweets=tweepy.Paginator(client.search_recent_tweets,query=query, tweet_fields=['context_annotations','created_at','public_metrics','author_id','geo'], \n",
    "                                                expansions= ['author_id','geo.place_id'],\n",
    "                                                #place_fields=['id','country','country_code','name'],\n",
    "                                                user_fields=['name','username','description','location','public_metrics','verified'],\n",
    "                                                #sort_order='relevancy',\n",
    "                                                max_results=100,limit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paged_results_df_cn_wk3 = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Iterate through each page of results\n",
    "for n,page in enumerate(tweets):  \n",
    "\n",
    "  # Print the current page number\n",
    "  print(\"PAGE NUMBER: \",n)\n",
    "  \n",
    "  # Iterate through each tweet in the page.data, convert to a dataframe, and \n",
    "  # then append to the paged_results_df above.\n",
    "  for tweet in page.data:\n",
    "    temp_df_cn_wk3 = pd.json_normalize(tweet.data, sep=\"_\")\n",
    "    paged_results_df_cn_wk3 = paged_results_df_cn_wk3.append(temp_df_cn_wk3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '(carbon footprint) OR #carbonfootprint -is:retweet lang=en'\n",
    "tweets=tweepy.Paginator(client.search_recent_tweets,query=query, tweet_fields=['context_annotations','created_at','public_metrics','author_id','geo'], \n",
    "                                                expansions= ['author_id','geo.place_id'],\n",
    "                                                #place_fields=['id','country','country_code','name'],\n",
    "                                                user_fields=['name','username','description','location','public_metrics','verified'],\n",
    "                                                #sort_order='relevancy',\n",
    "                                                max_results=100,limit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paged_results_df_cf_wk3 = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Iterate through each page of results\n",
    "for n,page in enumerate(tweets):  \n",
    "\n",
    "  # Print the current page number\n",
    "  print(\"PAGE NUMBER: \",n)\n",
    "  \n",
    "  # Iterate through each tweet in the page.data, convert to a dataframe, and \n",
    "  # then append to the paged_results_df above.\n",
    "  for tweet in page.data:\n",
    "    temp_df_cf_wk3= pd.json_normalize(tweet.data, sep=\"_\")\n",
    "    paged_results_df_cf_wk3 = paged_results_df_cf_wk3.append(temp_df_cf_wk3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paged_results_df_wk3.to_csv('30k_results_carbon_cred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "paged_results_df_test_wk3.to_csv('70k_results_net_zero.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70950 entries, 0 to 70949\n",
      "Data columns (total 15 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   author_id                     70950 non-null  object\n",
      " 1   context_annotations           38318 non-null  object\n",
      " 2   id                            70950 non-null  object\n",
      " 3   text                          70950 non-null  object\n",
      " 4   created_at                    70950 non-null  object\n",
      " 5   edit_history_tweet_ids        70950 non-null  object\n",
      " 6   public_metrics_retweet_count  70950 non-null  int64 \n",
      " 7   public_metrics_reply_count    70950 non-null  int64 \n",
      " 8   public_metrics_like_count     70950 non-null  int64 \n",
      " 9   public_metrics_quote_count    70950 non-null  int64 \n",
      " 10  geo_place_id                  172 non-null    object\n",
      " 11  geo_coordinates_type          1 non-null      object\n",
      " 12  geo_coordinates_coordinates   1 non-null      object\n",
      " 13  withheld_copyright            137 non-null    object\n",
      " 14  withheld_country_codes        137 non-null    object\n",
      "dtypes: int64(4), object(11)\n",
      "memory usage: 8.1+ MB\n"
     ]
    }
   ],
   "source": [
    "paged_results_df_test_wk3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "paged_results_df_cn_wk3.to_csv('5k_results_carbon_neutral.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "paged_results_df_cf_wk3.to_csv('16k_results_carbon_footprint.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('classification')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b161b1c3207380de6396d2091c7e58ae16163ea816ef25fea719df5fe370bcac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
